{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\\\n",
    "# Example usage:\n",
    "#\n",
    "# python train.py \\\n",
    "#   --logtostderr \\\n",
    "#   --data_dir ~/vid2depth/data/kitti_raw_eigen \\\n",
    "#   --seq_length 3 \\\n",
    "#   --reconstr_weight 0.85 \\\n",
    "#   --smooth_weight 0.05 \\\n",
    "#   --ssim_weight 0.15 \\\n",
    "#   --checkpoint_dir ~/vid2depth/checkpoints\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import model\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import util\n",
    "\n",
    "gfile = tf.gfile\n",
    "\n",
    "HOME_DIR = os.path.expanduser('~')\n",
    "DEFAULT_DATA_DIR = os.path.join(HOME_DIR, 'vid2depth/data/kitti_raw_eigen')\n",
    "DEFAULT_CHECKPOINT_DIR = os.path.join(HOME_DIR, 'vid2depth/checkpoints')\n",
    "flags.DEFINE_string('data_dir', DEFAULT_DATA_DIR, 'Preprocessed data.')\n",
    "flags.DEFINE_float('learning_rate', 0.0002, 'Adam learning rate.')\n",
    "flags.DEFINE_float('beta1', 0.9, 'Adam momentum.')\n",
    "flags.DEFINE_float('reconstr_weight', 0.85, 'Frame reconstruction loss weight.')\n",
    "flags.DEFINE_float('smooth_weight', 0.05, 'Smoothness loss weight.')\n",
    "flags.DEFINE_float('ssim_weight', 0.15, 'SSIM loss weight.')\n",
    "flags.DEFINE_float('icp_weight', 0.0, 'ICP loss weight.')\n",
    "flags.DEFINE_integer('batch_size', 4, 'The size of a sample batch')\n",
    "flags.DEFINE_integer('img_height', 128, 'Input frame height.')\n",
    "flags.DEFINE_integer('img_width', 416, 'Input frame width.')\n",
    "# Note: Training time grows linearly with sequence length.  Use 2 or 3.\n",
    "flags.DEFINE_integer('seq_length', 3, 'Number of frames in sequence.')\n",
    "flags.DEFINE_string('pretrained_ckpt', None, 'Path to checkpoint with '\n",
    "                    'pretrained weights.  Do not include .data* extension.')\n",
    "flags.DEFINE_string('checkpoint_dir', DEFAULT_CHECKPOINT_DIR,\n",
    "                    'Directory to save model checkpoints.')\n",
    "flags.DEFINE_integer('train_steps', 200000, 'Number of training steps.')\n",
    "flags.DEFINE_integer('summary_freq', 100, 'Save summaries every N steps.')\n",
    "flags.DEFINE_bool('legacy_mode', False, 'Whether to limit losses to using only '\n",
    "                  'the middle frame in sequence as the target frame.')\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Maximum number of checkpoints to keep.\n",
    "MAX_TO_KEEP = 100\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    # Fixed seed for repeatability\n",
    "    seed = 8964\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    if FLAGS.legacy_mode and FLAGS.seq_length < 3:\n",
    "        raise ValueError('Legacy mode supports sequence length > 2 only.')\n",
    "\n",
    "    if not gfile.Exists(FLAGS.checkpoint_dir):\n",
    "        gfile.MakeDirs(FLAGS.checkpoint_dir)\n",
    "\n",
    "    train_model = model.Model(data_dir=FLAGS.data_dir,\n",
    "                            is_training=True,\n",
    "                            learning_rate=FLAGS.learning_rate,\n",
    "                            beta1=FLAGS.beta1,\n",
    "                            reconstr_weight=FLAGS.reconstr_weight,\n",
    "                            smooth_weight=FLAGS.smooth_weight,\n",
    "                            ssim_weight=FLAGS.ssim_weight,\n",
    "                            icp_weight=FLAGS.icp_weight,\n",
    "                            batch_size=FLAGS.batch_size,\n",
    "                            img_height=FLAGS.img_height,\n",
    "                            img_width=FLAGS.img_width,\n",
    "                            seq_length=FLAGS.seq_length,\n",
    "                            legacy_mode=FLAGS.legacy_mode)\n",
    "\n",
    "    train(train_model, FLAGS.pretrained_ckpt, FLAGS.checkpoint_dir, FLAGS.train_steps, FLAGS.summary_freq)\n",
    "\n",
    "\n",
    "def train(train_model, pretrained_ckpt, checkpoint_dir, train_steps, summary_freq):\n",
    "    if pretrained_ckpt is not None:\n",
    "        vars_to_restore = util.get_vars_to_restore(pretrained_ckpt)\n",
    "        pretrain_restorer = tf.train.Saver(vars_to_restore)\n",
    "    vars_to_save = util.get_vars_to_restore()\n",
    "    saver = tf.train.Saver(vars_to_save + [train_model.global_step], max_to_keep=MAX_TO_KEEP)\n",
    "    sv = tf.train.Supervisor(logdir=checkpoint_dir, save_summaries_secs=0, saver=None)\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with sv.managed_session(config=config) as sess:\n",
    "        if pretrained_ckpt is not None:\n",
    "            logging.info('Restoring pretrained weights from %s', pretrained_ckpt)\n",
    "            pretrain_restorer.restore(sess, pretrained_ckpt)\n",
    "        logging.info('Attempting to resume training from %s...', checkpoint_dir)\n",
    "        checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        logging.info('Last checkpoint found: %s', checkpoint)\n",
    "        if checkpoint:\n",
    "            saver.restore(sess, checkpoint)\n",
    "\n",
    "        logging.info('Training...')\n",
    "        start_time = time.time()\n",
    "        last_summary_time = time.time()\n",
    "        steps_per_epoch = train_model.reader.steps_per_epoch\n",
    "        step = 1\n",
    "        while step <= train_steps:\n",
    "            fetches = {'train': train_model.train_op,\n",
    "                      'global_step': train_model.global_step,\n",
    "                      'incr_global_step': train_model.incr_global_step}\n",
    "\n",
    "            if step % summary_freq == 0:\n",
    "                fetches['loss'] = train_model.total_loss\n",
    "                fetches['summary'] = sv.summary_op\n",
    "\n",
    "            results = sess.run(fetches)\n",
    "            global_step = results['global_step']\n",
    "\n",
    "            if step % summary_freq == 0:\n",
    "                sv.summary_writer.add_summary(results['summary'], global_step)\n",
    "                train_epoch = math.ceil(global_step / steps_per_epoch)\n",
    "                train_step = global_step - (train_epoch - 1) * steps_per_epoch\n",
    "                this_cycle = time.time() - last_summary_time\n",
    "                last_summary_time += this_cycle\n",
    "                logging.info('Epoch: [%2d] [%5d/%5d] time: %4.2fs (%ds total) loss: %.3f',\n",
    "                            train_epoch, train_step, steps_per_epoch, this_cycle,\n",
    "                            time.time() - start_time, results['loss'])\n",
    "\n",
    "            if step % steps_per_epoch == 0:\n",
    "                logging.info('[*] Saving checkpoint to %s...', checkpoint_dir)\n",
    "                saver.save(sess, os.path.join(checkpoint_dir, 'model'), global_step=global_step)\n",
    "\n",
    "            # Setting step to global_step allows for training for a total of\n",
    "            # train_steps even if the program is restarted during training.\n",
    "            step = global_step + 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnparsedFlagAccessError",
     "evalue": "Trying to access flag --use_bn before flags were parsed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnparsedFlagAccessError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c0540ee98c2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-c0540ee98c2c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m     65\u001b[0m                             \u001b[0mimg_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                             \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                             legacy_mode=legacy_mode)\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_ckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/bong/vid2depth/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, is_training, learning_rate, beta1, reconstr_weight, smooth_weight, ssim_weight, icp_weight, batch_size, img_height, img_width, seq_length, legacy_mode)\u001b[0m\n\u001b[1;32m     79\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                                           self.seq_length, NUM_SCALES)\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_train_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_depth_test_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/bong/vid2depth/model.py\u001b[0m in \u001b[0;36mbuild_train_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_train_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_inference_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_train_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/bong/vid2depth/model.py\u001b[0m in \u001b[0;36mbuild_inference_for_training\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintrinsic_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintrinsic_mat_inv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'egomotion_prediction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0megomotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0megomotion_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegacy_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'depth_prediction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;31m# Organized by ...[i][scale].  Note that the order is flipped in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/bong/vid2depth/nets.py\u001b[0m in \u001b[0;36megomotion_net\u001b[0;34m(image_stack, is_training, legacy_mode)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pose_exp_net'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mend_points_collection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_end_points'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mnormalizer_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mnormalizer_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_norm_params\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;31m# get too much noise.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnparsedFlagAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnparsedFlagAccessError\u001b[0m: Trying to access flag --use_bn before flags were parsed."
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import model\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import util\n",
    "\n",
    "gfile = tf.gfile\n",
    "\n",
    "DEFAULT_DATA_DIR = '/data1/depth/bike/bike_pre'\n",
    "DEFAULT_CHECKPOINT_DIR = 'checkpoints'\n",
    "data_dir = DEFAULT_DATA_DIR\n",
    "learning_rate = 0.0002\n",
    "beta1 = 0.9\n",
    "reconstr_weight = 0.85\n",
    "smooth_weight = 0.05\n",
    "ssim_weight = 0.15\n",
    "batch_size = 4\n",
    "img_height = 128\n",
    "img_width = 416\n",
    "seq_length = 3\n",
    "pretrained_ckpt = None\n",
    "checkpoint_dir = DEFAULT_CHECKPOINT_DIR\n",
    "train_steps = 200000\n",
    "summary_freq = 100\n",
    "legacy_mode = False\n",
    "\n",
    "# Maximum number of checkpoints to keep.\n",
    "MAX_TO_KEEP = 100\n",
    "\n",
    "def main(_):\n",
    "    # Fixed seed for repeatability\n",
    "    seed = 8964\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    if legacy_mode and seq_length < 3:\n",
    "        raise ValueError('Legacy mode supports sequence length > 2 only.')\n",
    "\n",
    "    if not gfile.Exists(checkpoint_dir):\n",
    "        gfile.MakeDirs(checkpoint_dir)\n",
    "\n",
    "    train_model = model.Model(data_dir=data_dir,\n",
    "                            is_training=True,\n",
    "                            learning_rate=learning_rate,\n",
    "                            beta1=beta1,\n",
    "                            reconstr_weight=reconstr_weight,\n",
    "                            smooth_weight=smooth_weight,\n",
    "                            ssim_weight=ssim_weight,\n",
    "                            icp_weight=icp_weight,\n",
    "                            batch_size=batch_size,\n",
    "                            img_height=img_height,\n",
    "                            img_width=img_width,\n",
    "                            seq_length=seq_length,\n",
    "                            legacy_mode=legacy_mode)\n",
    "\n",
    "    train(train_model, pretrained_ckpt, checkpoint_dir, train_steps, summary_freq)\n",
    "\n",
    "\n",
    "def train(train_model, pretrained_ckpt, checkpoint_dir, train_steps, summary_freq):\n",
    "    if pretrained_ckpt is not None:\n",
    "        vars_to_restore = util.get_vars_to_restore(pretrained_ckpt)\n",
    "        pretrain_restorer = tf.train.Saver(vars_to_restore)\n",
    "    vars_to_save = util.get_vars_to_restore()\n",
    "    saver = tf.train.Saver(vars_to_save + [train_model.global_step], max_to_keep=MAX_TO_KEEP)\n",
    "    sv = tf.train.Supervisor(logdir=checkpoint_dir, save_summaries_secs=0, saver=None)\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with sv.managed_session(config=config) as sess:\n",
    "        if pretrained_ckpt is not None:\n",
    "            logging.info('Restoring pretrained weights from %s', pretrained_ckpt)\n",
    "            pretrain_restorer.restore(sess, pretrained_ckpt)\n",
    "        logging.info('Attempting to resume training from %s...', checkpoint_dir)\n",
    "        checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        logging.info('Last checkpoint found: %s', checkpoint)\n",
    "        if checkpoint:\n",
    "            saver.restore(sess, checkpoint)\n",
    "\n",
    "        logging.info('Training...')\n",
    "        start_time = time.time()\n",
    "        last_summary_time = time.time()\n",
    "        steps_per_epoch = train_model.reader.steps_per_epoch\n",
    "        step = 1\n",
    "        while step <= train_steps:\n",
    "            fetches = {'train': train_model.train_op,\n",
    "                      'global_step': train_model.global_step,\n",
    "                      'incr_global_step': train_model.incr_global_step}\n",
    "\n",
    "            if step % summary_freq == 0:\n",
    "                fetches['loss'] = train_model.total_loss\n",
    "                fetches['summary'] = sv.summary_op\n",
    "\n",
    "            results = sess.run(fetches)\n",
    "            global_step = results['global_step']\n",
    "\n",
    "            if step % summary_freq == 0:\n",
    "                sv.summary_writer.add_summary(results['summary'], global_step)\n",
    "                train_epoch = math.ceil(global_step / steps_per_epoch)\n",
    "                train_step = global_step - (train_epoch - 1) * steps_per_epoch\n",
    "                this_cycle = time.time() - last_summary_time\n",
    "                last_summary_time += this_cycle\n",
    "                logging.info('Epoch: [%2d] [%5d/%5d] time: %4.2fs (%ds total) loss: %.3f',\n",
    "                            train_epoch, train_step, steps_per_epoch, this_cycle,\n",
    "                            time.time() - start_time, results['loss'])\n",
    "\n",
    "            if step % steps_per_epoch == 0:\n",
    "                logging.info('[*] Saving checkpoint to %s...', checkpoint_dir)\n",
    "                saver.save(sess, os.path.join(checkpoint_dir, 'model'), global_step=global_step)\n",
    "\n",
    "            # Setting step to global_step allows for training for a total of\n",
    "            # train_steps even if the program is restarted during training.\n",
    "            step = global_step + 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
