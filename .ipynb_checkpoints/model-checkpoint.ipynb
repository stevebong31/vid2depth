{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Build model for inference or training.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import nets\n",
    "import project\n",
    "import reader\n",
    "import tensorflow as tf\n",
    "import util\n",
    "\n",
    "gfile = tf.gfile\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "NUM_SCALES = 4\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "    def __init__(self,\n",
    "               data_dir=None,\n",
    "               is_training=True,\n",
    "               learning_rate=0.0002,\n",
    "               beta1=0.9,\n",
    "               reconstr_weight=0.85,\n",
    "               smooth_weight=0.05,\n",
    "               ssim_weight=0.15,\n",
    "               icp_weight=0.0,\n",
    "               batch_size=4,\n",
    "               img_height=128,\n",
    "               img_width=416,\n",
    "               seq_length=3,\n",
    "               legacy_mode=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.is_training = is_training\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reconstr_weight = reconstr_weight\n",
    "        self.smooth_weight = smooth_weight\n",
    "        self.ssim_weight = ssim_weight\n",
    "        self.icp_weight = icp_weight\n",
    "        self.beta1 = beta1\n",
    "        self.batch_size = batch_size\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.seq_length = seq_length\n",
    "        self.legacy_mode = legacy_mode\n",
    "\n",
    "        if self.is_training:\n",
    "            self.reader = reader.DataReader(self.data_dir, self.batch_size,\n",
    "                                          self.img_height, self.img_width,\n",
    "                                          self.seq_length, NUM_SCALES)\n",
    "            self.build_train_graph()\n",
    "        else:\n",
    "            self.build_depth_test_graph()\n",
    "            self.build_egomotion_test_graph()\n",
    "\n",
    "        # At this point, the model is ready.  Print some info on model params.\n",
    "        util.count_parameters()\n",
    "\n",
    "    def build_train_graph(self):\n",
    "        self.build_inference_for_training()\n",
    "        self.build_loss()\n",
    "        self.build_train_op()\n",
    "        self.build_summaries()\n",
    "\n",
    "    def build_inference_for_training(self):\n",
    "        (self.image_stack, self.intrinsic_mat, self.intrinsic_mat_inv) = (self.reader.read_data())\n",
    "        with tf.name_scope('egomotion_prediction'):\n",
    "            self.egomotion, _ = nets.egomotion_net(self.image_stack, is_training=True, legacy_mode=self.legacy_mode)\n",
    "        with tf.variable_scope('depth_prediction'):\n",
    "            # Organized by ...[i][scale].  Note that the order is flipped in\n",
    "            # variables in build_loss() below.\n",
    "            self.disp = {}\n",
    "            self.depth = {}\n",
    "            for i in range(self.seq_length):\n",
    "                image = self.image_stack[:, :, :, 3 * i:3 * (i + 1)]\n",
    "                multiscale_disps_i, _ = nets.disp_net(image, is_training=True)\n",
    "                multiscale_depths_i = [1.0 / d for d in multiscale_disps_i]\n",
    "                self.disp[i] = multiscale_disps_i\n",
    "                self.depth[i] = multiscale_depths_i\n",
    "                # Reuse the same depth graph for all images.\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "    def build_loss(self):\n",
    "        \"\"\"Adds ops for computing loss.\"\"\"\n",
    "        with tf.name_scope('compute_loss'):\n",
    "            self.reconstr_loss = 0\n",
    "            self.smooth_loss = 0\n",
    "            self.ssim_loss = 0\n",
    "            # self.images is organized by ...[scale][B, h, w, seq_len * 3].\n",
    "            self.images = [{} for _ in range(NUM_SCALES)]\n",
    "            # Following nested lists are organized by ...[scale][source-target].\n",
    "            self.warped_image = [{} for _ in range(NUM_SCALES)]\n",
    "            self.warp_mask = [{} for _ in range(NUM_SCALES)]\n",
    "            self.warp_error = [{} for _ in range(NUM_SCALES)]\n",
    "            self.ssim_error = [{} for _ in range(NUM_SCALES)]\n",
    "\n",
    "            self.middle_frame_index = util.get_seq_middle(self.seq_length)\n",
    "\n",
    "            # Compute losses at each scale.\n",
    "            for s in range(NUM_SCALES):\n",
    "                # Scale image stack.\n",
    "                height_s = int(self.img_height / (2**s))\n",
    "                width_s = int(self.img_width / (2**s))\n",
    "                self.images[s] = tf.image.resize_area(self.image_stack,\n",
    "                                          [height_s, width_s])\n",
    "\n",
    "                # Smoothness.\n",
    "                if self.smooth_weight > 0:\n",
    "                    for i in range(self.seq_length):\n",
    "                        # In legacy mode, use the depth map from the middle frame only.\n",
    "                        if not self.legacy_mode or i == self.middle_frame_index:\n",
    "                            self.smooth_loss += 1.0 / (2**s) * self.depth_smoothness(\n",
    "                                self.disp[i][s], self.images[s][:, :, :, 3 * i:3 * (i + 1)])\n",
    "\n",
    "                for i in range(self.seq_length):\n",
    "                    for j in range(self.seq_length):\n",
    "                        # Only consider adjacent frames.\n",
    "                        if i == j or abs(i - j) != 1:\n",
    "                            continue\n",
    "                        # In legacy mode, only consider the middle frame as target.\n",
    "                        if self.legacy_mode and j != self.middle_frame_index:\n",
    "                            continue\n",
    "                        source = self.images[s][:, :, :, 3 * i:3 * (i + 1)]\n",
    "                        target = self.images[s][:, :, :, 3 * j:3 * (j + 1)]\n",
    "                        target_depth = self.depth[j][s]\n",
    "                        key = '%d-%d' % (i, j)\n",
    "\n",
    "                        # Extract ego-motion from i to j\n",
    "                        egomotion_index = min(i, j)\n",
    "                        egomotion_mult = 1\n",
    "                        if i > j:\n",
    "                            # Need to inverse egomotion when going back in sequence.\n",
    "                            egomotion_mult *= -1\n",
    "                            # For compatiblity with SfMLearner, interpret all egomotion vectors\n",
    "                            # as pointing toward the middle frame.  Note that unlike SfMLearner,\n",
    "                            # each vector captures the motion to/from its next frame, and not\n",
    "                            # the center frame.  Although with seq_length == 3, there is no\n",
    "                            # difference.\n",
    "                        if self.legacy_mode:\n",
    "                            if egomotion_index >= self.middle_frame_index:\n",
    "                                egomotion_mult *= -1\n",
    "                        egomotion = egomotion_mult * self.egomotion[:, egomotion_index, :]\n",
    "\n",
    "                        # Inverse warp the source image to the target image frame for\n",
    "                        # photometric consistency loss.\n",
    "                        self.warped_image[s][key], self.warp_mask[s][key] = (project.inverse_warp(source, \n",
    "                                                                                                 target_depth, \n",
    "                                                                                                 egomotion, \n",
    "                                                                                                 self.intrinsic_mat[:, s, :, :], \n",
    "                                                                                                 self.intrinsic_mat_inv[:, s, :, :]))\n",
    "\n",
    "                        # Reconstruction loss.\n",
    "                        self.warp_error[s][key] = tf.abs(self.warped_image[s][key] - target)\n",
    "                        self.reconstr_loss += tf.reduce_mean(self.warp_error[s][key] * self.warp_mask[s][key])\n",
    "                        # SSIM.\n",
    "                        if self.ssim_weight > 0:\n",
    "                            self.ssim_error[s][key] = self.ssim(self.warped_image[s][key], target)\n",
    "                            # TODO(rezama): This should be min_pool2d().\n",
    "                            ssim_mask = slim.avg_pool2d(self.warp_mask[s][key], 3, 1, 'VALID')\n",
    "                            self.ssim_loss += tf.reduce_mean(self.ssim_error[s][key] * ssim_mask)\n",
    "\n",
    "            self.total_loss = self.reconstr_weight * self.reconstr_loss\n",
    "            if self.smooth_weight > 0:\n",
    "                self.total_loss += self.smooth_weight * self.smooth_loss\n",
    "            if self.ssim_weight > 0:\n",
    "                self.total_loss += self.ssim_weight * self.ssim_loss\n",
    "\n",
    "    def gradient_x(self, img):\n",
    "        return img[:, :, :-1, :] - img[:, :, 1:, :]\n",
    "\n",
    "    def gradient_y(self, img):\n",
    "        return img[:, :-1, :, :] - img[:, 1:, :, :]\n",
    "\n",
    "    def depth_smoothness(self, depth, img):\n",
    "        \"\"\"Computes image-aware depth smoothness loss.\"\"\"\n",
    "        depth_dx = self.gradient_x(depth)\n",
    "        depth_dy = self.gradient_y(depth)\n",
    "        image_dx = self.gradient_x(img)\n",
    "        image_dy = self.gradient_y(img)\n",
    "        weights_x = tf.exp(-tf.reduce_mean(tf.abs(image_dx), 3, keepdims=True))\n",
    "        weights_y = tf.exp(-tf.reduce_mean(tf.abs(image_dy), 3, keepdims=True))\n",
    "        smoothness_x = depth_dx * weights_x\n",
    "        smoothness_y = depth_dy * weights_y\n",
    "        return tf.reduce_mean(abs(smoothness_x)) + tf.reduce_mean(abs(smoothness_y))\n",
    "\n",
    "    def ssim(self, x, y):\n",
    "        \"\"\"Computes a differentiable structured image similarity measure.\"\"\"\n",
    "        c1 = 0.01**2\n",
    "        c2 = 0.03**2\n",
    "        mu_x = slim.avg_pool2d(x, 3, 1, 'VALID')\n",
    "        mu_y = slim.avg_pool2d(y, 3, 1, 'VALID')\n",
    "        sigma_x = slim.avg_pool2d(x**2, 3, 1, 'VALID') - mu_x**2\n",
    "        sigma_y = slim.avg_pool2d(y**2, 3, 1, 'VALID') - mu_y**2\n",
    "        sigma_xy = slim.avg_pool2d(x * y, 3, 1, 'VALID') - mu_x * mu_y\n",
    "        ssim_n = (2 * mu_x * mu_y + c1) * (2 * sigma_xy + c2)\n",
    "        ssim_d = (mu_x**2 + mu_y**2 + c1) * (sigma_x + sigma_y + c2)\n",
    "        ssim = ssim_n / ssim_d\n",
    "        return tf.clip_by_value((1 - ssim) / 2, 0, 1)\n",
    "\n",
    "    def build_train_op(self):\n",
    "        with tf.name_scope('train_op'):\n",
    "            optim = tf.train.AdamOptimizer(self.learning_rate, self.beta1)\n",
    "            self.train_op = slim.learning.create_train_op(self.total_loss, optim)\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            self.incr_global_step = tf.assign(self.global_step, self.global_step + 1)\n",
    "\n",
    "    def build_summaries(self):\n",
    "        \"\"\"Adds scalar and image summaries for TensorBoard.\"\"\"\n",
    "        tf.summary.scalar('total_loss', self.total_loss)\n",
    "        tf.summary.scalar('reconstr_loss', self.reconstr_loss)\n",
    "        if self.smooth_weight > 0:\n",
    "            tf.summary.scalar('smooth_loss', self.smooth_loss)\n",
    "        if self.ssim_weight > 0:\n",
    "            tf.summary.scalar('ssim_loss', self.ssim_loss)\n",
    "\n",
    "        for i in range(self.seq_length - 1):\n",
    "            tf.summary.histogram('tx%d' % i, self.egomotion[:, i, 0])\n",
    "            tf.summary.histogram('ty%d' % i, self.egomotion[:, i, 1])\n",
    "            tf.summary.histogram('tz%d' % i, self.egomotion[:, i, 2])\n",
    "            tf.summary.histogram('rx%d' % i, self.egomotion[:, i, 3])\n",
    "            tf.summary.histogram('ry%d' % i, self.egomotion[:, i, 4])\n",
    "            tf.summary.histogram('rz%d' % i, self.egomotion[:, i, 5])\n",
    "\n",
    "        for s in range(NUM_SCALES):\n",
    "            for i in range(self.seq_length):\n",
    "                tf.summary.image('scale%d_image%d' % (s, i), self.images[s][:, :, :, 3 * i:3 * (i + 1)])\n",
    "            if i in self.depth:\n",
    "                tf.summary.histogram('scale%d_depth%d' % (s, i), self.depth[i][s])\n",
    "                tf.summary.histogram('scale%d_disp%d' % (s, i), self.disp[i][s])\n",
    "                tf.summary.image('scale%d_disparity%d' % (s, i), self.disp[i][s])\n",
    "\n",
    "            for key in self.warped_image[s]:\n",
    "                tf.summary.image('scale%d_warped_image%s' % (s, key), self.warped_image[s][key])\n",
    "                tf.summary.image('scale%d_warp_mask%s' % (s, key), self.warp_mask[s][key])\n",
    "                tf.summary.image('scale%d_warp_error%s' % (s, key), self.warp_error[s][key])\n",
    "                if self.ssim_weight > 0:\n",
    "                    tf.summary.image('scale%d_ssim_error%s' % (s, key), self.ssim_error[s][key])\n",
    "\n",
    "    def build_depth_test_graph(self):\n",
    "        with tf.name_scope('depth_prediction'):\n",
    "            with tf.variable_scope('depth_prediction'):\n",
    "                input_uint8 = tf.placeholder(tf.uint8, [self.batch_size, self.img_height, self.img_width, 3],mname='raw_input')\n",
    "                input_float = tf.image.convert_image_dtype(input_uint8, tf.float32)\n",
    "                # TODO(rezama): Retrain published model with batchnorm params and set\n",
    "                # is_training to False.\n",
    "                est_disp, _ = nets.disp_net(input_float, is_training=True)\n",
    "                est_depth = 1.0 / est_disp[0]\n",
    "        self.inputs_depth = input_uint8\n",
    "        self.est_depth = est_depth\n",
    "\n",
    "    def build_egomotion_test_graph(self):\n",
    "        input_uint8 = tf.placeholder(tf.uint8, [self.batch_size, self.img_height, self.img_width * self.seq_length, 3], name='raw_input')\n",
    "        input_float = tf.image.convert_image_dtype(input_uint8, tf.float32)\n",
    "        image_seq = input_float\n",
    "        image_stack = self.unpack_image_batches(image_seq)\n",
    "        with tf.name_scope('egomotion_prediction'):\n",
    "        # TODO(rezama): Retrain published model with batchnorm params and set\n",
    "        # is_training to False.\n",
    "            egomotion, _ = nets.egomotion_net(image_stack, is_training=True, legacy_mode=self.legacy_mode)\n",
    "        self.inputs_egomotion = input_uint8\n",
    "        self.est_egomotion = egomotion\n",
    "\n",
    "    def unpack_image_batches(self, image_seq):\n",
    "        with tf.name_scope('unpack_images'):\n",
    "            image_list = [image_seq[:, :, i * self.img_width:(i + 1) * self.img_width, :]for i in range(self.seq_length)]\n",
    "            image_stack = tf.concat(image_list, axis=3)\n",
    "            image_stack.set_shape([self.batch_size, self.img_height, self.img_width, self.seq_length * 3])\n",
    "        return image_stack\n",
    "\n",
    "    def inference(self, inputs, sess, mode):\n",
    "        fetches = {}\n",
    "        if mode == 'depth':\n",
    "            fetches['depth'] = self.est_depth\n",
    "            inputs_ph = self.inputs_depth\n",
    "        if mode == 'egomotion':\n",
    "            fetches['egomotion'] = self.est_egomotion\n",
    "            inputs_ph = self.inputs_egomotion\n",
    "        results = sess.run(fetches, feed_dict={inputs_ph: inputs})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
