{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nets.py\n",
    "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Depth and Ego-Motion networks.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import util\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "# TODO(rezama): Move flag to main, pass as argument to functions.\n",
    "flags.DEFINE_bool('use_bn', True, 'Add batch norm layers.')\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Weight regularization.\n",
    "WEIGHT_REG = 0.05\n",
    "\n",
    "# Disparity (inverse depth) values range from 0.01 to 10.\n",
    "DISP_SCALING = 10\n",
    "MIN_DISP = 0.01\n",
    "\n",
    "EGOMOTION_VEC_SIZE = 6\n",
    "\n",
    "\n",
    "def egomotion_net(image_stack, is_training=True, legacy_mode=False):\n",
    "    \"\"\"Predict ego-motion vectors from a stack of frames.\n",
    "    Args:\n",
    "    image_stack: Input tensor with shape [B, h, w, seq_length * 3].  Regardless\n",
    "        of the value of legacy_mode, the input image sequence passed to the\n",
    "        function should be in normal order, e.g. [1, 2, 3].\n",
    "    is_training: Whether the model is being trained or not.\n",
    "    legacy_mode: Setting legacy_mode to True enables compatibility with\n",
    "        SfMLearner checkpoints.  When legacy_mode is on, egomotion_net()\n",
    "        rearranges the input tensor to place the target (middle) frame first in\n",
    "        sequence.  This is the arrangement of inputs that legacy models have\n",
    "        received during training.  In legacy mode, the client program\n",
    "        (model.Model.build_loss()) interprets the outputs of this network\n",
    "        differently as well.  For example:\n",
    "        When legacy_mode == True,\n",
    "        Network inputs will be [2, 1, 3]\n",
    "        Network outputs will be [1 -> 2, 3 -> 2]\n",
    "        When legacy_mode == False,\n",
    "        Network inputs will be [1, 2, 3]\n",
    "        Network outputs will be [1 -> 2, 2 -> 3]\n",
    "    Returns:\n",
    "    Egomotion vectors with shape [B, seq_length - 1, 6].\n",
    "    \"\"\"\n",
    "    seq_length = image_stack.get_shape()[3].value // 3  # 3 == RGB.\n",
    "    if legacy_mode:\n",
    "    # Put the target frame at the beginning of stack.\n",
    "        with tf.name_scope('rearrange_stack'):\n",
    "            mid_index = util.get_seq_middle(seq_length)\n",
    "            left_subset = image_stack[:, :, :, :mid_index * 3]\n",
    "            target_frame = image_stack[:, :, :, mid_index * 3:(mid_index + 1) * 3]\n",
    "            right_subset = image_stack[:, :, :, (mid_index + 1) * 3:]\n",
    "            image_stack = tf.concat([target_frame, left_subset, right_subset], axis=3)\n",
    "\n",
    "    batch_norm_params = {'is_training': is_training}\n",
    "    num_egomotion_vecs = seq_length - 1\n",
    "    with tf.variable_scope('pose_exp_net') as sc:\n",
    "        end_points_collection = sc.original_name_scope + '_end_points'\n",
    "        normalizer_fn = slim.batch_norm if True else None\n",
    "        normalizer_params = batch_norm_params if True else None\n",
    "        with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
    "                            normalizer_fn=normalizer_fn,\n",
    "                            weights_regularizer=slim.l2_regularizer(WEIGHT_REG),\n",
    "                            normalizer_params=normalizer_params,\n",
    "                            activation_fn=tf.nn.relu,\n",
    "                            outputs_collections=end_points_collection):\n",
    "            cnv1 = slim.conv2d(image_stack, 16, [7, 7], stride=2, scope='cnv1')\n",
    "            cnv2 = slim.conv2d(cnv1, 32, [5, 5], stride=2, scope='cnv2')\n",
    "            cnv3 = slim.conv2d(cnv2, 64, [3, 3], stride=2, scope='cnv3')\n",
    "            cnv4 = slim.conv2d(cnv3, 128, [3, 3], stride=2, scope='cnv4')\n",
    "            cnv5 = slim.conv2d(cnv4, 256, [3, 3], stride=2, scope='cnv5')\n",
    "\n",
    "      # Ego-motion specific layers\n",
    "        with tf.variable_scope('pose'):\n",
    "            cnv6 = slim.conv2d(cnv5, 256, [3, 3], stride=2, scope='cnv6')\n",
    "            cnv7 = slim.conv2d(cnv6, 256, [3, 3], stride=2, scope='cnv7')\n",
    "            pred_channels = EGOMOTION_VEC_SIZE * num_egomotion_vecs\n",
    "            egomotion_pred = slim.conv2d(cnv7,\n",
    "                                     pred_channels,\n",
    "                                     [1, 1],\n",
    "                                     scope='pred',\n",
    "                                     stride=1,\n",
    "                                     normalizer_fn=None,\n",
    "                                     activation_fn=None)\n",
    "            egomotion_avg = tf.reduce_mean(egomotion_pred, [1, 2])\n",
    "            # Tinghui found that scaling by a small constant facilitates training.\n",
    "            egomotion_final = 0.01 * tf.reshape(egomotion_avg, [-1, num_egomotion_vecs, EGOMOTION_VEC_SIZE])\n",
    "\n",
    "        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "        return egomotion_final, end_points\n",
    "\n",
    "\n",
    "def disp_net(target_image, is_training=True):\n",
    "    batch_norm_params = {'is_training': is_training}\n",
    "    h = target_image.get_shape()[1].value\n",
    "    w = target_image.get_shape()[2].value\n",
    "    inputs = target_image\n",
    "    with tf.variable_scope('depth_net') as sc:\n",
    "        end_points_collection = sc.original_name_scope + '_end_points'\n",
    "        normalizer_fn = slim.batch_norm if True else None\n",
    "        normalizer_params = batch_norm_params if True else None\n",
    "        with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
    "                            normalizer_fn=normalizer_fn,\n",
    "                            normalizer_params=normalizer_params,\n",
    "                            weights_regularizer=slim.l2_regularizer(WEIGHT_REG),\n",
    "                            activation_fn=tf.nn.relu,\n",
    "                            outputs_collections=end_points_collection):\n",
    "            cnv1 = slim.conv2d(inputs, 32, [7, 7], stride=2, scope='cnv1')\n",
    "            cnv1b = slim.conv2d(cnv1, 32, [7, 7], stride=1, scope='cnv1b')\n",
    "            cnv2 = slim.conv2d(cnv1b, 64, [5, 5], stride=2, scope='cnv2')\n",
    "            cnv2b = slim.conv2d(cnv2, 64, [5, 5], stride=1, scope='cnv2b')\n",
    "\n",
    "            cnv3 = slim.conv2d(cnv2b, 128, [3, 3], stride=2, scope='cnv3')\n",
    "            cnv3b = slim.conv2d(cnv3, 128, [3, 3], stride=1, scope='cnv3b')\n",
    "            cnv4 = slim.conv2d(cnv3b, 256, [3, 3], stride=2, scope='cnv4')\n",
    "            cnv4b = slim.conv2d(cnv4, 256, [3, 3], stride=1, scope='cnv4b')\n",
    "            cnv5 = slim.conv2d(cnv4b, 512, [3, 3], stride=2, scope='cnv5')\n",
    "            cnv5b = slim.conv2d(cnv5, 512, [3, 3], stride=1, scope='cnv5b')\n",
    "            cnv6 = slim.conv2d(cnv5b, 512, [3, 3], stride=2, scope='cnv6')\n",
    "            cnv6b = slim.conv2d(cnv6, 512, [3, 3], stride=1, scope='cnv6b')\n",
    "            cnv7 = slim.conv2d(cnv6b, 512, [3, 3], stride=2, scope='cnv7')\n",
    "            cnv7b = slim.conv2d(cnv7, 512, [3, 3], stride=1, scope='cnv7b')\n",
    "\n",
    "            up7 = slim.conv2d_transpose(cnv7b, 512, [3, 3], stride=2, scope='upcnv7')\n",
    "            # There might be dimension mismatch due to uneven down/up-sampling.\n",
    "            up7 = _resize_like(up7, cnv6b)\n",
    "            i7_in = tf.concat([up7, cnv6b], axis=3)\n",
    "            icnv7 = slim.conv2d(i7_in, 512, [3, 3], stride=1, scope='icnv7')\n",
    "\n",
    "            up6 = slim.conv2d_transpose(icnv7, 512, [3, 3], stride=2, scope='upcnv6')\n",
    "            up6 = _resize_like(up6, cnv5b)\n",
    "            i6_in = tf.concat([up6, cnv5b], axis=3)\n",
    "            icnv6 = slim.conv2d(i6_in, 512, [3, 3], stride=1, scope='icnv6')\n",
    "\n",
    "            up5 = slim.conv2d_transpose(icnv6, 256, [3, 3], stride=2, scope='upcnv5')\n",
    "            up5 = _resize_like(up5, cnv4b)\n",
    "            i5_in = tf.concat([up5, cnv4b], axis=3)\n",
    "            icnv5 = slim.conv2d(i5_in, 256, [3, 3], stride=1, scope='icnv5')\n",
    "\n",
    "            up4 = slim.conv2d_transpose(icnv5, 128, [3, 3], stride=2, scope='upcnv4')\n",
    "            i4_in = tf.concat([up4, cnv3b], axis=3)\n",
    "            icnv4 = slim.conv2d(i4_in, 128, [3, 3], stride=1, scope='icnv4')\n",
    "            disp4 = (slim.conv2d(icnv4, 1, [3, 3], stride=1, activation_fn=tf.sigmoid,\n",
    "                               normalizer_fn=None, scope='disp4')\n",
    "                   * DISP_SCALING + MIN_DISP)\n",
    "            disp4_up = tf.image.resize_bilinear(disp4, [np.int(h / 4), np.int(w / 4)])\n",
    "\n",
    "            up3 = slim.conv2d_transpose(icnv4, 64, [3, 3], stride=2, scope='upcnv3')\n",
    "            i3_in = tf.concat([up3, cnv2b, disp4_up], axis=3)\n",
    "            icnv3 = slim.conv2d(i3_in, 64, [3, 3], stride=1, scope='icnv3')\n",
    "            disp3 = (slim.conv2d(icnv3, 1, [3, 3], stride=1, activation_fn=tf.sigmoid,\n",
    "                               normalizer_fn=None, scope='disp3')\n",
    "                   * DISP_SCALING + MIN_DISP)\n",
    "            disp3_up = tf.image.resize_bilinear(disp3, [np.int(h / 2), np.int(w / 2)])\n",
    "\n",
    "            up2 = slim.conv2d_transpose(icnv3, 32, [3, 3], stride=2, scope='upcnv2')\n",
    "            i2_in = tf.concat([up2, cnv1b, disp3_up], axis=3)\n",
    "            icnv2 = slim.conv2d(i2_in, 32, [3, 3], stride=1, scope='icnv2')\n",
    "            disp2 = (slim.conv2d(icnv2, 1, [3, 3], stride=1, activation_fn=tf.sigmoid,\n",
    "                               normalizer_fn=None, scope='disp2')\n",
    "                   * DISP_SCALING + MIN_DISP)\n",
    "            disp2_up = tf.image.resize_bilinear(disp2, [h, w])\n",
    "\n",
    "            up1 = slim.conv2d_transpose(icnv2, 16, [3, 3], stride=2, scope='upcnv1')\n",
    "            i1_in = tf.concat([up1, disp2_up], axis=3)\n",
    "            icnv1 = slim.conv2d(i1_in, 16, [3, 3], stride=1, scope='icnv1')\n",
    "            disp1 = (slim.conv2d(icnv1, 1, [3, 3], stride=1, activation_fn=tf.sigmoid,\n",
    "                               normalizer_fn=None, scope='disp1')\n",
    "                   * DISP_SCALING + MIN_DISP)\n",
    "\n",
    "            end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "            return [disp1, disp2, disp3, disp4], end_points\n",
    "\n",
    "\n",
    "def _resize_like(inputs, ref):\n",
    "    i_h, i_w = inputs.get_shape()[1], inputs.get_shape()[2]\n",
    "    r_h, r_w = ref.get_shape()[1], ref.get_shape()[2]\n",
    "    if i_h == r_h and i_w == r_w:\n",
    "        return inputs\n",
    "    else:\n",
    "        return tf.image.resize_nearest_neighbor(inputs, [r_h.value, r_w.value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
